#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=1

#SBATCH --job-name=NLP-SST2
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --time=06:59:00
#SBATCH --mem=60000M
#SBATCH --output=/home/scur1382/nlp_practicals_2023/%A.out

date

export TMP_DIR="/scratch-local/scur1382"

module purge
module load 2022
module load Anaconda3/2022.05

# Your job starts in the directory where you call sbatch
WORK_DIR=$HOME/nlp_practicals_2023
cd $WORK_DIR

# Activate your environment
source activate dl2023

# Run 1: Visual prompting CLIP on CIFAR-10 with standard text prompt
code_dir=$WORK_DIR/practical2

## Set of experiments on CIFAR-10
root=$TMP_DIR
mkdir -p $root

cp -r $WORK_DIR/data/ $TMP_DIR/

seeds=(0 42 69)

# Run with BOW
for seed in "${seeds[@]}"
do
    python $code_dir/bow.py \
        --epochs 30 \
        --seed $seed \
        --data_dir $root \
        --lr 0.003 \
        --epochs 30 \
        --embeddings_type glove
        # --debug
done

# Run with own embeddings
for seed in "${seeds[@]}"
do
    python $code_dir/cbow.py \
        --epochs 30 \
        --seed $seed \
        --data_dir $root \
        --lr 0.003 \
        --epochs 30 \
        --embeddings_type glove
        # --debug
done

# Run with own embeddings (deep)
for seed in "${seeds[@]}"
do
    python $code_dir/deepcbow.py \
        --epochs 30 \
        --seed $seed \
        --data_dir $root \
        --lr 0.003 \
        --epochs 30 \
        --embeddings_type glove
        # --debug
done

# Run with deep CBOW with pretrained embeddings
for seed in "${seeds[@]}"
do
    python $code_dir/deepcbow_pt.py \
        --epochs 30 \
        --seed $seed \
        --data_dir $root \
        --lr 0.003 \
        --epochs 30 \
        --embeddings_type glove
        # --debug
done

for seed in "${seeds[@]}"
do
    python $code_dir/lstm.py \
        --epochs 30 \
        --seed $seed \
        --data_dir $root \
        --lr 0.003 \
        --epochs 30 \
        --embeddings_type glove
        # --debug
done

# Run with fine-tunable embeddings
for seed in "${seeds[@]}"
do
    python $code_dir/lstm.py \
        --epochs 30 \
        --seed $seed \
        --data_dir $root \
        --lr 0.003 \
        --epochs 30 \
        --embeddings_type glove \
        --train_embeddings
        # --debug
done


for seed in "${seeds[@]}"
do
    python $code_dir/treelstm.py \
        --epochs 30 \
        --seed $seed \
        --data_dir $root \
        --lr 0.003 \
        --epochs 30 \
        --embeddings_type glove
        # --debug
done